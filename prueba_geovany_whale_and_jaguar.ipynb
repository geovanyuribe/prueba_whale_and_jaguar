{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos las librerías necesarias\n",
    "\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from textblob import TextBlob\n",
    "import re\n",
    "import time\n",
    "import string\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from textacy.viz.termite import draw_termite_plot\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder, LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense, Dropout, LSTM, Embedding\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import np_utils\n",
    "from keras.layers import Dense, Input, LSTM, Bidirectional, Activation, Conv1D, GRU, TimeDistributed\n",
    "from keras.layers import Dropout, Embedding, GlobalMaxPooling1D, MaxPooling1D, Add, Flatten, SpatialDropout1D\n",
    "from keras.layers import GlobalAveragePooling1D, BatchNormalization, concatenate\n",
    "from keras.layers import Reshape, merge, Concatenate, Lambda, Average\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "from keras import initializers, regularizers, constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos el dataframe (en local en mi caso)\n",
    "\n",
    "df = pd.read_json('News_Category_Dataset_v2.json', lines=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PRE-PROCESAMIENTO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primero observamos que hay una categoría duplicada, así que las unimos\n",
    "\n",
    "df['category'] = df['category'].map(lambda x: \"WORLDPOST\" if x == \"THE WORLDPOST\" else x)\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# La forma en que utilizaremos los titulares y la descripción es uniendo ambos strings y analizando sobre éste\n",
    "\n",
    "df['text'] = df['headline'] + \" \" + df['short_description']\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Estas dos funciones limpian el texto, lo tokeniza, quita las stop words, las palabras de 2 o menos caracteres,\n",
    "# los no alfanuméricos y lematiza\n",
    "\n",
    "stop_words_ = set(stopwords.words('english'))\n",
    "wn = WordNetLemmatizer()\n",
    "my_sw = ['make', 'amp',  'news','new' ,'time', 'u','s', 'photos',  'get', 'say']\n",
    "\n",
    "def black_txt(token):\n",
    "    return  token not in stop_words_ and token not in list(string.punctuation)  and len(token)>2 and token not in my_sw\n",
    "\n",
    "def clean_txt(text):\n",
    "    clean_text = []\n",
    "    clean_text2 = []\n",
    "    text = re.sub(\"'\", \"\",text)\n",
    "    text=re.sub(\"(\\\\d|\\\\W)+\",\" \",text)    \n",
    "    clean_text = [ wn.lemmatize(word, pos=\"v\") for word in word_tokenize(text.lower()) if black_txt(word)]\n",
    "    clean_text2 = [word for word in clean_text if black_txt(word)]\n",
    "    return \" \".join(clean_text2)\n",
    "    \n",
    "#-------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Función para el cálculo de la polaridad basado en TextBlob\n",
    "\n",
    "def polarity_txt(text):\n",
    "    return TextBlob(text).sentiment[0]\n",
    "\n",
    "# Creamos la nueva columna de polaridad\n",
    "\n",
    "df['polarity'] = df['text'].apply(polarity_txt)\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Función para el cálculo de la subjetividad basado en TextBlob\n",
    "\n",
    "def subj_txt(text):\n",
    "    return  TextBlob(text).sentiment[1]\n",
    "\n",
    "# Creamos la nueva columna de subjetividad\n",
    "\n",
    "df['subjectivity'] = df['text'].apply(subj_txt)\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Función para calcular la proporción del texto limpio con respecto al texto normal\n",
    "\n",
    "def len_text(text):\n",
    "    if len(text.split())>0:\n",
    "        return len(set(clean_txt(text).split()))/ len(text.split())\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "# Creamos la nueva columna con len\n",
    "\n",
    "df['len'] = df['text'].apply(len_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RETO 1:\n",
    "### ¿Se pueden catalogar las noticias con la descripción y los titulares? Compara tu clasificación con las categorías incluidas en el set de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos las clases para el transformer y la extracción de estadísticas del texto\n",
    "\n",
    "class ItemSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, data_dict):\n",
    "        return data_dict[self.key]\n",
    "\n",
    "\n",
    "class TextStats(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, data):\n",
    "        return [{'pos':  row['polarity'], 'sub': row['subjectivity'],  'len': row['len']} for _, row in data.iterrows()]\n",
    "    \n",
    "#-------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Creamos el flujo de procesamiento\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('union', FeatureUnion(\n",
    "        transformer_list=[\n",
    "\n",
    "            # Esta parte preprocesa el texto plano según la función de limpieza de texto que definimos y crea matrices TF-IDF\n",
    "            ('text', Pipeline([\n",
    "                ('selector', ItemSelector(key='text')),\n",
    "                ('tfidf', TfidfVectorizer( min_df =3, max_df=0.2, max_features=None, \n",
    "                    strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',\n",
    "                    ngram_range=(1, 10), use_idf=1,smooth_idf=1,sublinear_tf=1,\n",
    "                    stop_words = None, preprocessor=clean_txt)),\n",
    "            ])),\n",
    "\n",
    "            # Aquí se extraen características de los meta datos\n",
    "            ('stats', Pipeline([\n",
    "                ('selector', ItemSelector(key=['polarity', 'subjectivity', 'len'])),\n",
    "                ('stats', TextStats()),\n",
    "                ('vect', DictVectorizer()),\n",
    "            ])),\n",
    "\n",
    "        ],\n",
    "\n",
    "        # Los pesos en la unión de los transformers\n",
    "        transformer_weights={\n",
    "            'text': 0.9,\n",
    "            'stats': 1.5,\n",
    "        },\n",
    "    ))\n",
    "])\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Aplicamos el flujo de procesamiento a la base, con una validación del 20%\n",
    "\n",
    "seed = 40\n",
    "X = df[['text', 'polarity', 'subjectivity','len']]\n",
    "y = df['category']\n",
    "encoder = LabelEncoder()\n",
    "y = encoder.fit_transform(y)\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed, stratify=y)\n",
    "pipeline.fit(x_train)\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Descargamos el modelo a utilizar y lo instanciamos\n",
    "\n",
    "# Utilizamos el modelo grande de spacy, !python -m spacy download en_core_web_lg en caso de no tenerlo descargado\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "# Indicamos los encoders\n",
    "\n",
    "X = df['text']\n",
    "y = df['category']\n",
    "encoder = LabelEncoder()\n",
    "y = encoder.fit_transform(y)\n",
    "Y = np_utils.to_categorical(y)\n",
    "\n",
    "# Creamos los vectores con tf-idf (Term Frequency-Inverse Document Frequency)\n",
    "\n",
    "vectorizer = TfidfVectorizer( min_df =3, max_df=0.2, max_features=None, \n",
    "            strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',\n",
    "            use_idf=1,smooth_idf=1,sublinear_tf=1,\n",
    "            stop_words = None, preprocessor=clean_txt)\n",
    "\n",
    "seed = 40\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=seed, stratify =y)\n",
    "vectorizer.fit(x_train)\n",
    "\n",
    "word2idx = {word: idx for idx, word in enumerate(vectorizer.get_feature_names())}\n",
    "tokenize = vectorizer.build_tokenizer()\n",
    "preprocess = vectorizer.build_preprocessor()\n",
    " \n",
    "def to_sequence(tokenizer, preprocessor, index, text):\n",
    "    words = tokenizer(preprocessor(text))\n",
    "    indexes = [index[word] for word in words if word in index]\n",
    "    return indexes\n",
    "\n",
    "X_train_sequences = [to_sequence(tokenize, preprocess, word2idx, x) for x in x_train]\n",
    "\n",
    "print(X_train_sequences[0])\n",
    "\n",
    "MAX_SEQ_LENGHT=60\n",
    "\n",
    "N_FEATURES = len(vectorizer.get_feature_names())\n",
    "X_train_sequences = pad_sequences(X_train_sequences, maxlen=MAX_SEQ_LENGHT, value=N_FEATURES)\n",
    "X_test_sequences = [to_sequence(tokenize, preprocess, word2idx, x) for x in x_test]\n",
    "X_test_sequences = pad_sequences(X_test_sequences, maxlen=MAX_SEQ_LENGHT, value=N_FEATURES)\n",
    "\n",
    "EMBEDDINGS_LEN = 300\n",
    "\n",
    "embeddings_index = np.zeros((len(vectorizer.get_feature_names()) + 1, EMBEDDINGS_LEN))\n",
    "for word, idx in word2idx.items():\n",
    "    try:\n",
    "        embedding = nlp.vocab[word].vector\n",
    "        embeddings_index[idx] = embedding\n",
    "    except:\n",
    "        pass\n",
    "      \n",
    "#-------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Inicializamos el modelo, lo entrenamos (con 5 épocas) y probamos su precisión\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(vectorizer.get_feature_names()) + 1,\n",
    "                    EMBEDDINGS_LEN\n",
    "                    weights=[embeddings_index],\n",
    "                    input_length=MAX_SEQ_LENGHT,\n",
    "                    trainable=False))\n",
    "model.add(LSTM(300, dropout=0.2))\n",
    "model.add(Dense(len(set(y)), activation='softmax'))\n",
    " \n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "model.fit(X_train_sequences, y_train, \n",
    "          epochs=5, batch_size=128, verbose=1, \n",
    "          validation_split=0.1)\n",
    " \n",
    "scores = model.evaluate(X_test_sequences, y_test, verbose=1)\n",
    "print(\"Accuracy:\", scores[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RETO 2: \n",
    "### ¿Existen estilos de escritura asociados a cada categoría?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función que calcula la riqueza léxica (entendida como la cantidad de palabras únicas sobre la cantidad de palabras totales)\n",
    "\n",
    "def lexical_richness(text):\n",
    "    try:\n",
    "        vocabulary = sorted(np.unique(nltk.tokenize.word_tokenize(text)))\n",
    "        return len(vocabulary)/len(nltk.tokenize.word_tokenize(text))\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "# Creamos la nueva columna de riqueza léxica y excluimos los que no tenían título ni descripción (riqueza léxica 0)\n",
    "\n",
    "df['lexical_richness'] = df['text'].apply(lexical_richness)\n",
    "df = df[df['lexical_richness']!=0]\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Vectorizamos el texto (de nuevo utilizando tf-idf)\n",
    "\n",
    "vectorizer = TfidfVectorizer( min_df =3, max_df=0.2, max_features=None, \n",
    "                    strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',\n",
    "                    ngram_range=(1, 1), use_idf=1,smooth_idf=1,sublinear_tf=1,\n",
    "                    stop_words = None, preprocessor=clean_txt)\n",
    "\n",
    "vectorizer.fit(df.category)\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Creamos el top de palabras\n",
    "\n",
    "def create_tf_matrix(category):\n",
    "    return vectorizer.transform(df[df.category == category].text)\n",
    "\n",
    "def create_term_freq(matrix, cat):\n",
    "    category_words = matrix.sum(axis=0)\n",
    "    category_words_freq = [(word, category_words[0, idx]) for word, idx in vectorizer.vocabulary_.items()]\n",
    "    return pd.DataFrame(list(sorted(category_words_freq, key = lambda x: x[1], reverse=True)),columns=['Terms', cat])\n",
    "\n",
    "for cat in df.category.unique():\n",
    "    df_right = create_term_freq(create_tf_matrix(cat), cat).head(5)\n",
    "    if cat != 'CRIME':\n",
    "        df_top5_words = df_top5_words.merge(df_right, how='outer')\n",
    "    else:\n",
    "        df_top5_words = df_right.copy()\n",
    "        \n",
    "df_top5_words.fillna(0, inplace=True )\n",
    "df_top5_words.set_index('Terms', inplace=True)\n",
    "df_top5_words.shape\n",
    "\n",
    "df_2 = df_top5_words.copy()\n",
    "df_norm = (df_2) / (df_2.max() - df_2.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gráfico con las 5 palabras más usadas por categoría\n",
    "\n",
    "draw_termite_plot(np.array(df_norm.values), df_top5_words.columns, df_top5_words.index, highlight_cols=[], save='reto_2_words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Gráfico del sentimiento por categoría\n",
    "\n",
    "plt.figure(figsize=(20,11))\n",
    "ax = sns.boxplot(x=\"category\", y=\"polarity\", data=df)\n",
    "ax.set_title('Sentimiento por categorías')\n",
    "l = ax.set_xticklabels(ax.get_xticklabels(), rotation=60)\n",
    "plt.savefig('reto_2_sentimiento')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gráfico de la subjetividad por categoría\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "ax = sns.boxplot(x=\"category\", y=\"subjectivity\", data=df)\n",
    "ax.set_title('Subjetividad por categorías')\n",
    "l= ax.set_xticklabels(ax.get_xticklabels(), rotation=90)\n",
    "plt.savefig('reto_2_subjetividad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gráfaico de la riqueza léxica por categoría\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "ax = sns.boxplot(x=\"category\", y=\"lexical_richness\", data=df)\n",
    "ax.set_title('Riqueza léxica por categorías')\n",
    "l= ax.set_xticklabels(ax.get_xticklabels(), rotation=90)\n",
    "plt.savefig('reto_2_lexical_richness')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RETO 3: \n",
    "### ¿Qué se puede decir de los autores a partir de los datos?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos un dataframe agregando por autor (si aparecen junto a otros se considera como un autor diferente)\n",
    "\n",
    "df['qty']= 1\n",
    "df_authors = df.groupby(['authors']).agg({'polarity': 'mean', \n",
    "                                     'subjectivity': 'mean', \n",
    "                                     'len': 'mean',\n",
    "                                     'lexical_richness':'mean',\n",
    "                                     'qty':'count'}).reset_index()\n",
    "\n",
    "df_authors.drop([0], axis=0, inplace =True)\n",
    "df_authors.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Top autores más profílicos (con más noticias)\n",
    "\n",
    "df_prolific = df_authors.sort_values(['qty'], ascending=False).head(10)\n",
    "\n",
    "def plot_grouped_bar(df, title):\n",
    "    barWidth = 0.25\n",
    "\n",
    "    # Definimos las variables (la riqueza léxica se divide en 2 para estar por debajo de 0.5 y graficarse mejor junto a las otras)\n",
    "    bars1 = df.polarity\n",
    "    bars2 = df.subjectivity\n",
    "    bars3 = df.lexical_richness/2\n",
    "\n",
    "    # Definimos las posiciones de las barras en el eje x\n",
    "    r1 = np.arange(len(bars1))\n",
    "    r2 = [x + barWidth for x in r1]\n",
    "    r3 = [x + barWidth for x in r2]\n",
    "    \n",
    "\n",
    "    # Definimos colores, etiquetas, etc\n",
    "    plt.figure(figsize=(15,10))\n",
    "    plt.bar(r1, bars1, color='#F08080', width=barWidth, edgecolor='white', label='Sentimiento')\n",
    "    plt.bar(r2, bars2, color='#B0C4DE', width=barWidth, edgecolor='white', label='Subjetividad')\n",
    "    plt.bar(r3, bars3, color='#BDECB6', width=barWidth, edgecolor='white', label='Riqueza léxica')\n",
    "\n",
    "    # Demás configuraciones del gráfico como título, ejes, etc\n",
    "    plt.title(title)\n",
    "    plt.xlabel('group', fontweight='bold')\n",
    "    plt.xticks([r + barWidth for r in range(len(bars1))], df.authors)\n",
    "    plt.xticks(rotation=60)\n",
    "    plt.legend()\n",
    "    plt.savefig(f'reto_3_{title}')\n",
    "    plt.show()\n",
    "\n",
    "plot_grouped_bar(df_prolific, \"Sentimiento, Subjetividad y Riqueza léxica de los autores más prolíficos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 5 autores más positivos y top 5 autores más negativos\n",
    "\n",
    "df_polarity = df_authors[df_authors.qty >200].sort_values(['polarity'], ascending=False).head(5).append(df_authors[df_authors.qty >200].sort_values(['polarity'], ascending=True).head(5).iloc[::-1])\n",
    "plot_grouped_bar(df_polarity, \"Sentimiento, Subjetividad y Riqueza léxica de los autores más positivos (Top 5 alto) y más negativos (Top 5 bajo) con más de 200 publicaciones\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 5 autores más subjetivos y top 5 autores más objetivos\n",
    "\n",
    "df_subjectivity = df_authors[df_authors.qty >200].sort_values(['subjectivity'], ascending=False).head(5).append(df_authors[df_authors.qty >200].sort_values(['subjectivity'], ascending=True).head(5).iloc[::-1])\n",
    "plot_grouped_bar(df_subjectivity, \"Sentimiento, Subjetividad y Riqueza léxica de los autores más subjetivos (Top 5 alto) y más objetivos (Top 5 bajo) con más de 200 publicaciones\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 5 autores con más riqueza léxica y top 5 con menor riqueza léxica\n",
    "\n",
    "df_lexical_richness = df_authors[df_authors.qty >200].sort_values(['lexical_richness'], ascending=False).head(5).append(df_authors[df_authors.qty >200].sort_values(['lexical_richness'], ascending=True).head(5).iloc[::-1])\n",
    "plot_grouped_bar(df_lexical_richness, \"Sentimiento, Subjetividad y Riqueza léxica de los autores con más riqueza léxica (Top 5 alto) y con menor (Top 5 bajo) con más de 200 publicaciones\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RETO 4:\n",
    "### Ahora, utilizando técnicas de aprendizaje no supervisado, trata de identificar temas, “protagonistas” u otras entidades de las noticias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Tomamos como referencia para la extracción de entidades las 100 noticias más largas\n",
    "\n",
    "df['n_c'] = df['text'].apply(len)\n",
    "df_100 = df.sort_values(['n_c'], ascending= False).head(100)\n",
    "\n",
    "# Para hacerlo más sencillo, creamos un string que suma los 100 textos\n",
    "\n",
    "string_text = ''\n",
    "for i in df_100['text']:\n",
    "    string_text = string_text + ' ' + i\n",
    "    \n",
    "# Utilizamos de nuevo el modelo grande de spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "# Creamos un dataframe con las entidades encontradas\n",
    "\n",
    "doc = nlp(string_text)\n",
    "entities = []\n",
    "labels = []\n",
    "position_start = []\n",
    "position_end = []\n",
    "\n",
    "for ent in doc.ents:\n",
    "    entities.append(ent) # La entidad encontrada\n",
    "    labels.append(ent.label_) # Etiqueta de la entidad\n",
    "    position_start.append(ent.start_char) # Posición del caracter inicial de la entidad en el string completo\n",
    "    position_end.append(ent.end_char) # Posición del caracter final de la entidad en el string completo\n",
    "    \n",
    "df_entities = pd.DataFrame({'Entities': entities, 'Labels':labels, 'Position_start':position_start, 'Position_end': position_end})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explicación de todos los tipos de entidades encontradas\n",
    "\n",
    "for i in df_entities['Labels'].unique():\n",
    "    print(i, ': ', spacy.explain(f'{i}'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RETO 5:\n",
    "### Basándote en el texto de la descripción corta, caracteriza este dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos un nuevo dataframe teniendo en cuenta solo la descripción corta\n",
    "\n",
    "df_short_description = df[['short_description']]\n",
    "df_short_description = df_short_description[df_short_description['short_description']!='']\n",
    "print(f'El {len(df_short_description)/len(df)*100}% de las noticias tienen una descipción corta')\n",
    "df_short_description\n",
    "\n",
    "# Calculamos la polaridad (solo teniendo en cuenta la descripción corta) y calculamos sus terciles\n",
    "\n",
    "df_short_description['polarity'] = df_short_description['short_description'].apply(polarity_txt)\n",
    "tercil_polarity = pd.qcut(df_short_description['polarity'], 3, labels=['Low', 'Medium', 'High'])\n",
    "df_short_description = df_short_description.assign(tercil_polarity=tercil_polarity.values)\n",
    "\n",
    "# Calculamos la subjetividad (solo teniendo en cuenta la descripción corta) y calculamos sus terciles\n",
    "\n",
    "df_short_description['subjectivity'] = df_short_description['short_description'].apply(subj_txt)\n",
    "tercil_subjectivity = pd.qcut(df_short_description['subjectivity'], 3, labels=['Low', 'Medium', 'High'])\n",
    "df_short_description = df_short_description.assign(tercil_subjectivity=tercil_subjectivity.values)\n",
    "\n",
    "# Calculamos la riqueza léxica (solo teniendo en cuenta la descripción corta) y calculamos sus extremos\n",
    "\n",
    "df_short_description['lexical_richness'] = df_short_description['short_description'].apply(lexical_richness)\n",
    "two_lexical_richness = pd.qcut(df_short_description['lexical_richness'], 2, labels=['Medium', 'High'])\n",
    "df_short_description = df_short_description.assign(two_lexical_richness=two_lexical_richness.values)\n",
    "\n",
    "df_short_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tienden a ser más objetivas aquellas noticias con mayor riqueza léxica\n",
    "\n",
    "by_two_lexical_richness_subjectivity = df_short_description.groupby('two_lexical_richness')['tercil_subjectivity'].value_counts(normalize=True)\n",
    "by_two_lexical_richness_subjectivity.unstack().reindex(columns=['Low', 'Medium', 'High']).plot(kind='bar', title='Subjetividad por riqueza léxica', stacked=False)\n",
    "plt.savefig(f'reto_5_subjetividad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tienden a ser más negativas aquellas noticias con mayor riqueza léxica\n",
    "\n",
    "by_two_lexical_richness_polarity = df_short_description.groupby('two_lexical_richness')['tercil_polarity'].value_counts(normalize=True)\n",
    "by_two_lexical_richness_polarity.unstack().reindex(columns=['Low', 'Medium', 'High']).plot(kind='bar', title='Sentimiento por riqueza léxica', stacked=False)\n",
    "plt.savefig(f'reto_5_sentimiento')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
